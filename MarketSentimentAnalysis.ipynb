{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eb34e-d685-48dd-acb6-c279981721c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, AdamW, GPT2Config\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356955f7-7371-4575-b69d-d1d4ad0d96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/content/drive/MyDrive/Market-Sentiment-Analysis/Stock-Market-News-Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4fb79-e96d-40a3-88f0-5568ea8794cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d994b-b52f-4863-b413-59ad8712445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1e346-2bb5-499f-a577-9cdeb3e0a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "  df['Sentence'] = df['Sentence'].str.lower() # convert all text to lower\n",
    "  df['Sentence'] = df['Sentence'].str.replace(r\"https?://\\S+|www\\.\\S+\",\" \",regex = True) # remove all URLs\n",
    "  df['Sentence'] = df['Sentence'].str.replace(r\"#[A-Za-z0-9_]+\",\" \", regex = True) #remove all hashtags\n",
    "  df['Sentence'] = df['Sentence'].str.replace(r\"@\",\"at\", regex = True) #replacing @ with at\n",
    "  df['Sentence'] = df['Sentence'].str.replace(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\",\" \", regex = True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961e5ff-667e-428e-8e4b-96bc7fb9bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742980ce-0fbf-4c8a-941b-00ccc6c2c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8b20b-71a5-489d-a7ba-f4229a9c528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a00a53-250d-4528-bef7-1811db0b66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mapping between classes and numerical labels\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Class Mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599f177-88cc-4ff5-a37c-1534e2165c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "documents = data['Sentence']\n",
    "labels = data['Sentiment']\n",
    "\n",
    "X_train, X_Rem, y_train, y_Rem = train_test_split(documents, labels, test_size= 1 - train_size, random_state=45, stratify = labels)\n",
    "\n",
    "# Split the remaining set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_Rem, y_Rem, test_size=test_size/(val_size + test_size), random_state=45, stratify = y_Rem)\n",
    "\n",
    "print(\"X_train size: \", X_train.size)\n",
    "print(\"y_train size: \", y_train.size)\n",
    "print(\"X_val size: \", X_val.size)\n",
    "print(\"y_val size: \", y_val.size)\n",
    "print(\"X_test size: \", X_test.size)\n",
    "print(\"y_test size: \", y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058733-8bd2-42b9-b871-f615a4bcb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketSentimentDataset(Dataset):\n",
    "    def __init__(self, documents, labels, tokenizer, max_len):\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) :\n",
    "        text = self.documents.iloc[idx]\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Squeeze to remove the extra dimension added by return_tensors='pt'\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a1836-99de-425f-ad5c-2a64730518bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "MAX_LEN = 256\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MarketSentimentDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = MarketSentimentDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "test_dataset = MarketSentimentDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GPT2ForSequenceClassificationWithDropout(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, dropout_prob: float):\n",
    "        super(GPT2ForSequenceClassificationWithDropout, self).__init__()\n",
    "        self.gpt2 = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.dropout(outputs.logits)\n",
    "        return logits\n",
    "\n",
    "dropout_prob = 0.3\n",
    "model = GPT2ForSequenceClassificationWithDropout(model_name, num_labels=3, dropout_prob=dropout_prob).to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * 3   # Assuming 2 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 3\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating Validation Accuracy:\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')  # Save the best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0e57e-4504-4473-8ebb-2f803324034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting:\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Perform the forward pass to get logits\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "        # Store the true labels and predictions\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy or other metrics if needed\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# If you want to see the predictions\n",
    "# print(\"Predictions:\", test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe099bb2-bb6b-422f-899d-33bd2dc80eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_text(model, tokenizer, text, max_len, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(inputs['attention_mask']).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted_label = torch.max(logits, 1)\n",
    "\n",
    "    return predicted_label.item()\n",
    "\n",
    "# Example usage\n",
    "text = \"XYZ company loses a copyyright lawsuit to pay a fine of $1 million\"\n",
    "predicted_label = predict_single_text(model, tokenizer, text, MAX_LEN, device)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
